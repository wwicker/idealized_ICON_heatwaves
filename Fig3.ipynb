{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ff7113-4717-44d8-a664-ace4a7f0cf79",
   "metadata": {},
   "source": [
    "# Figure 3\n",
    "\n",
    "estimating CI for metrics takes 2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64345b2-dc33-4bdd-84c3-a78c6c3df5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "import os\n",
    "import cmocean\n",
    "import pandas as pd\n",
    "import numba\n",
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "from icon_util import regrid\n",
    "\n",
    "work = os.environ['WORK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2209398d-092c-4df1-b741-08a17cda212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53388a1b-c2d0-4c8e-a8ec-1ec69dd09e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e1e4bc-770d-4deb-b732-bebca81616d9",
   "metadata": {},
   "source": [
    "## Heatwave metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e816e0a1-9b7c-40ec-b213-1da0bbfa4673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(data,dist,nseason=50):\n",
    "    '''\n",
    "        Produce Dataset with heatwave metrics from DataFrame and temperature distribution\n",
    "    '''\n",
    "    frequency = data.groupby('ncells')['length'].sum().to_xarray()\n",
    "    frequency = frequency / nseason\n",
    "\n",
    "    length = data.groupby('ncells')['length'].mean().to_xarray()\n",
    "\n",
    "    ds = xr.Dataset(dict(frequency=frequency,length=length))\n",
    "    ds = ds.assign_coords(dict(clon=dist.clon,clat=dist.clat))\n",
    "\n",
    "    return ds\n",
    "\n",
    "@numba.guvectorize(\n",
    "    \"(float64[:],float64[:],float64[:])\",\n",
    "    \"(n), (m) -> (m)\",\n",
    "    forceobj=True\n",
    ")    \n",
    "def icdf(a,p,out):\n",
    "    '''\n",
    "        Inverse empirical cummulative distribution function of array at percentiles p\n",
    "    '''\n",
    "    sort = np.sort(a)\n",
    "    out[:] = sort[np.int64(p*len(a))]\n",
    "\n",
    "\n",
    "def confid(dist,alpha):\n",
    "    '''\n",
    "        Estimate confidence intervals using the inverse empirical cummulative distribution function \n",
    "        for distrubution of bootstrap samples.\n",
    "    '''\n",
    "    p = xr.DataArray([alpha/2, 1-alpha/2],dims=('percentile'))\n",
    "    values = xr.apply_ufunc(icdf,\n",
    "                          *(dist,p),\n",
    "                          input_core_dims=[['random'],['percentile']],\n",
    "                          output_core_dims=[['percentile']],\n",
    "                          dask='parallelized',\n",
    "                          output_dtypes=[[dist.dtype]])\n",
    "    values['percentile'] = p\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b68eb9-4a42-4c75-8342-b9f47cdcdfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = work+'/wolfgang/icon_postproc_t1000_mean/atm_heldsuarez_default_bootstrap_p0.1/'\n",
    "files = [directory+f for f in os.listdir(directory) if f.startswith('sample')]\n",
    "files.sort()\n",
    "\n",
    "bootstrap = xr.open_mfdataset(files,combine='nested',concat_dim='random').chunk(dict(random=100))\n",
    "bootstrap = regrid(bootstrap,lim=(0,90))\n",
    "bootstrap = bootstrap.mean('longitude').compute()\n",
    "\n",
    "frequency = confid(bootstrap['frequency'],0.05) * 4\n",
    "length = confid(bootstrap['length'],0.05)\n",
    "\n",
    "exp_CI = xr.Dataset(dict(frequency=frequency,length=length))\n",
    "\n",
    "dist = xr.open_dataarray(work+'/wolfgang/icon_postproc_t1000_mean/atm_heldsuarez_default_t1000_mean_percentiles.nc')\n",
    "data = pd.read_json(work+'/wolfgang/icon_postproc_t1000_mean/atm_heldsuarez_default_t1000_mean_heatwaves.json')\n",
    "exp = metrics(data,dist)\n",
    "exp = regrid(exp,lim=(0,90))\n",
    "exp = exp.mean('longitude').compute()\n",
    "\n",
    "xr.Dataset(dict(frequency=exp['frequency'],length=exp['length'],frequency_CI=exp_CI['frequency'],length_CI=exp_CI['length'])).to_netcdf('./heatwaves_ref_t1000_mean.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad57b8-9fdd-4eb0-9cb7-734323fbf6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = work+'/wolfgang/icon_postproc_t1000_mean/atm_heldsuarez_butler_exp9_bootstrap_p0.1/'\n",
    "files = [directory+f for f in os.listdir(directory) if f.startswith('sample')]\n",
    "files.sort()\n",
    "\n",
    "bootstrap = xr.open_mfdataset(files,combine='nested',concat_dim='random').chunk(dict(random=100))\n",
    "bootstrap = regrid(bootstrap,lim=(0,90))\n",
    "bootstrap = bootstrap.mean('longitude').compute()\n",
    "\n",
    "frequency = confid(bootstrap['frequency'],0.05) * 4\n",
    "length = confid(bootstrap['length'],0.05)\n",
    "\n",
    "exp_CI = xr.Dataset(dict(frequency=frequency,length=length))\n",
    "\n",
    "dist = xr.open_dataarray(work+'/wolfgang/icon_postproc_t1000_mean/atm_heldsuarez_butler_exp9_t1000_mean_percentiles.nc')\n",
    "data = pd.read_json(work+'/wolfgang/icon_postproc_t1000_mean/atm_heldsuarez_butler_exp9_t1000_mean_heatwaves.json')\n",
    "exp = metrics(data,dist)\n",
    "exp = regrid(exp,lim=(0,90))\n",
    "exp = exp.mean('longitude').compute()\n",
    "\n",
    "xr.Dataset(dict(frequency=exp['frequency'],length=exp['length'],frequency_CI=exp_CI['frequency'],length_CI=exp_CI['length'])).to_netcdf('./heatwaves_exp9_t1000_mean.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b44e6-b222-4689-a7ab-1d9c8a341ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = work+'/wolfgang/icon_postproc_t1000_mean/atm_heldsuarez_butler_exp4_bootstrap_p0.1/'\n",
    "files = [directory+f for f in os.listdir(directory) if f.startswith('sample')]\n",
    "files.sort()\n",
    "\n",
    "bootstrap = xr.open_mfdataset(files,combine='nested',concat_dim='random').chunk(dict(random=100))\n",
    "bootstrap = regrid(bootstrap,lim=(0,90))\n",
    "bootstrap = bootstrap.mean('longitude').compute()\n",
    "\n",
    "frequency = confid(bootstrap['frequency'],0.05) * 4\n",
    "length = confid(bootstrap['length'],0.05)\n",
    "\n",
    "exp_CI = xr.Dataset(dict(frequency=frequency,length=length))\n",
    "\n",
    "dist = xr.open_dataarray(work+'/wolfgang/icon_postproc_t1000_mean/atm_heldsuarez_butler_exp4_t1000_mean_percentiles.nc')\n",
    "data = pd.read_json(work+'/wolfgang/icon_postproc_t1000_mean/atm_heldsuarez_butler_exp4_t1000_mean_heatwaves.json')\n",
    "exp = metrics(data,dist)\n",
    "exp = regrid(exp,lim=(0,90))\n",
    "exp = exp.mean('longitude').compute()\n",
    "\n",
    "xr.Dataset(dict(frequency=exp['frequency'],length=exp['length'],frequency_CI=exp_CI['frequency'],length_CI=exp_CI['length'])).to_netcdf('./heatwaves_exp4_t1000_mean.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a0be77-21ab-4f5c-9d9b-04dc0bf6a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = work+'/icon_postproc_t1000_mean/atm_heldsuarez_butler_exp8_bootstrap_p0.1/'\n",
    "files = [directory+f for f in os.listdir(directory) if f.startswith('sample')]\n",
    "files.sort()\n",
    "\n",
    "bootstrap = xr.open_mfdataset(files,combine='nested',concat_dim='random').chunk(dict(random=100))\n",
    "bootstrap = regrid(bootstrap,lim=(0,90))\n",
    "bootstrap = bootstrap.mean('longitude').compute()\n",
    "\n",
    "frequency = confid(bootstrap['frequency'],0.05) * 4\n",
    "length = confid(bootstrap['length'],0.05)\n",
    "\n",
    "exp_CI = xr.Dataset(dict(frequency=frequency,length=length))\n",
    "\n",
    "dist = xr.open_dataarray(work+'/wolfgang/icon_postproc_t1000_mean/atm_heldsuarez_butler_exp8_t1000_mean_percentiles.nc')\n",
    "data = pd.read_json(work+'/wolfgang/icon_postproc_t1000_mean/atm_heldsuarez_butler_exp8_t1000_mean_heatwaves.json')\n",
    "exp = metrics(data,dist)\n",
    "exp = regrid(exp,lim=(0,90))\n",
    "exp = exp.mean('longitude').compute()\n",
    "\n",
    "xr.Dataset(dict(frequency=exp['frequency'],length=exp['length'],frequency_CI=exp_CI['frequency'],length_CI=exp_CI['length'])).to_netcdf('./heatwaves_exp8_t1000_mean.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420cb988-ec90-4c9b-858b-039f7b5a80be",
   "metadata": {},
   "source": [
    "## Hot day persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a55663-38a8-4dec-9613-5002632cdf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def count_duration(array,index):\n",
    "    '''\n",
    "        Count occurence of set of consecutive hot days with certain length\n",
    "        \n",
    "        - first element of occurence counts sets that are longer than max_duration\n",
    "    '''\n",
    "    max_duration = 14\n",
    "    occurence = np.zeros(max_duration+1,np.int_)\n",
    "    count = 0\n",
    "    \n",
    "    for i in index:\n",
    "        if array[i]:\n",
    "            if count > 0:\n",
    "                if count > max_duration:\n",
    "                    occurence[0] += 1\n",
    "                else:\n",
    "                    occurence[count] += 1\n",
    "            \n",
    "            count = 0  \n",
    "            \n",
    "        else:\n",
    "            count +=1\n",
    "            \n",
    "    return occurence\n",
    "\n",
    "\n",
    "\n",
    "def loop_cells(array):\n",
    "    '''\n",
    "        Loop counter over ncells\n",
    "    '''\n",
    "    N = len(array.time)\n",
    "    index = np.arange(N)\n",
    "    \n",
    "    # prepare array\n",
    "    len1 = len(count_duration(array.isel(ncells=0).values,index))\n",
    "    hist = np.zeros((len1,len(array.ncells)),np.int_)\n",
    "    \n",
    "    for i in range(len(array.ncells)):\n",
    "        \n",
    "        hist[:,i] = count_duration(array.isel(ncells=i).values,index)\n",
    "        \n",
    "        \n",
    "    # prepare data\n",
    "    length = xr.DataArray(range(1,len1),dims=('length'))\n",
    "    events = xr.DataArray(hist[1:,:],dims=('length','ncells'),coords=dict(length=length))\n",
    "    \n",
    "    days = length * events\n",
    "    \n",
    "    missing_events = xr.DataArray(hist[0,:],dims='ncells')\n",
    "    missing_days = 0.1*N - days.sum('length')\n",
    "    \n",
    "    return xr.Dataset(dict(days=days,missing=missing_days,missing_events=missing_events))\n",
    "\n",
    "\n",
    "\n",
    "def non_hot_days(paths,dist):\n",
    "    \n",
    "    # load temperature data\n",
    "    array = xr.open_mfdataset(paths,combine='nested',concat_dim='time')['ta'].squeeze()\n",
    "    array = array.sel(time=slice(1300,501300))\n",
    "    \n",
    "    # compute daily means\n",
    "    array['time'] = array['time'].astype(np.int64)\n",
    "    array = array.groupby('time').reduce(np.mean)\n",
    "    array = array.compute()\n",
    "    \n",
    "    # identify days that fail to exceed the threshold\n",
    "    failing = (array < dist.sel(p=0.90).squeeze())\n",
    "    \n",
    "    return failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef45454-5971-44d3-8477-a5160b202cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = work+'/DATA/icon_simulations/atm_heldsuarez_default/3d/ta/100000pa/atm_heldsuarez_default_pl_*.nc'\n",
    "\n",
    "dist = xr.open_dataarray(work+'/wolfgang/icon_postproc_t1000_mean/atm_heldsuarez_default_t1000_mean_percentiles.nc')\n",
    "\n",
    "failing = non_hot_days(paths,dist)\n",
    "\n",
    "counts_ref = loop_cells(failing)\n",
    "\n",
    "counts_ref = regrid(counts_ref.assign_coords(dict(clon=dist.clon,clat=dist.clat)),lim=(0,90)).mean('longitude')\n",
    "\n",
    "counts_ref['days'] = counts_ref['days'] / 50\n",
    "\n",
    "counts_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc820431-5eaa-4bf0-a096-7874fbbc9374",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = work+'/DATA/icon_simulations/atm_heldsuarez_butler_exp9/3d/ta/100000pa/atm_heldsuarez_butler_exp9_pl_*.nc'\n",
    "\n",
    "dist = xr.open_dataarray(work+'/wolfgang/icon_postproc_t1000_mean/atm_heldsuarez_butler_exp9_t1000_mean_percentiles.nc')\n",
    "\n",
    "failing = non_hot_days(paths,dist)\n",
    "\n",
    "counts_exp9 = loop_cells(failing)\n",
    "\n",
    "counts_exp9 = regrid(counts_exp9.assign_coords(dict(clon=dist.clon,clat=dist.clat)),lim=(0,90)).mean('longitude')\n",
    "\n",
    "counts_exp9['days'] = counts_exp9['days'] / 50\n",
    "\n",
    "counts_exp9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd677fd-f0f6-4757-a638-507fedae67b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = work+'/DATA/icon_simulations/atm_heldsuarez_butler_exp4/3d/ta/100000pa/atm_heldsuarez_butler_exp4_pl_*.nc'\n",
    "\n",
    "dist = xr.open_dataarray(work+'/wolfgang/icon_postproc_t1000_mean/atm_heldsuarez_butler_exp4_t1000_mean_percentiles.nc')\n",
    "\n",
    "failing = non_hot_days(paths,dist)\n",
    "\n",
    "counts_exp4 = loop_cells(failing)\n",
    "\n",
    "counts_exp4 = regrid(counts_exp4.assign_coords(dict(clon=dist.clon,clat=dist.clat)),lim=(0,90)).mean('longitude')\n",
    "\n",
    "counts_exp4['days'] = counts_exp4['days'] / 50\n",
    "\n",
    "counts_exp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1074a45c-d92f-4d86-9a6c-df6c6ddd6e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = work+'/wolfgang/icon_postproc_t1000_mean/atm_heldsuarez_default_hot_persistence_bootstrap_p0.03_regrid/'\n",
    "files = [directory+f for f in os.listdir(directory) if f.startswith('regrid')]\n",
    "files.sort()\n",
    "\n",
    "bootstrap = xr.open_mfdataset(files,combine='nested',concat_dim='random')['days'].compute()\n",
    "\n",
    "CI_ref = confid(bootstrap,0.05)\n",
    "CI_ref = CI_ref / 50\n",
    "\n",
    "CI_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83018a0-861c-485e-b145-703f001bbc29",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b7b4b-ac77-4927-927a-6b29dd42da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = xr.open_dataset('./heatwaves_ref_t1000_mean.nc')\n",
    "exp9 = xr.open_dataset('./heatwaves_exp9_t1000_mean.nc')\n",
    "exp4 = xr.open_dataset('./heatwaves_exp4_t1000_mean.nc')\n",
    "exp8 = xr.open_dataset('./heatwaves_exp8_t1000_mean.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e56a52-ae12-4a32-9c60-fcc413432070",
   "metadata": {},
   "outputs": [],
   "source": [
    "stormtrack_latitude = lambda da: da['latitude'].isel(latitude=da.argmax('latitude'))\n",
    "\n",
    "directory = work+'/wolfgang/icon_storm_track/'\n",
    "eke = xr.open_dataset(directory+'atm_heldsuarez_default_EKE_zonal_mean_10day_highpass.nc')['EKE'].sel(latitude=slice(0,90))\n",
    "ref_latitude = stormtrack_latitude(eke.mean('time'))\n",
    "\n",
    "directory = work+'/wolfgang/icon_storm_track/'\n",
    "eke = xr.open_dataset(directory+'atm_heldsuarez_butler_exp9_EKE_zonal_mean_10day_highpass.nc')['EKE'].sel(latitude=slice(0,90))\n",
    "exp9_latitude = stormtrack_latitude(eke.mean('time'))\n",
    "\n",
    "\n",
    "directory = work+'/wolfgang/icon_storm_track/'\n",
    "eke = xr.open_dataset(directory+'atm_heldsuarez_butler_exp4_EKE_zonal_mean_10day_highpass.nc')['EKE'].sel(latitude=slice(0,90))\n",
    "exp4_latitude = stormtrack_latitude(eke.mean('time'))\n",
    "\n",
    "directory = work+'/wolfgang/icon_storm_track/'\n",
    "eke = xr.open_dataset(directory+'atm_heldsuarez_butler_exp8_EKE_zonal_mean_10day_highpass.nc')['EKE'].sel(latitude=slice(0,90))\n",
    "exp8_latitude = stormtrack_latitude(eke.mean('time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd40dafe-efa3-464f-98a2-6b5136b86b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2,ncols=2,figsize=(8,6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Frequency\n",
    "l1 = ref['frequency'].plot(ax=axes[0],linestyle='-')\n",
    "l2 = exp9['frequency'].plot(ax=axes[0],linestyle='--')\n",
    "l3 = exp4['frequency'].plot(ax=axes[0],linestyle=':')\n",
    "l4 = exp8['frequency'].plot(ax=axes[0],linestyle='-.')\n",
    "\n",
    "axes[0].fill_between(ref['latitude'].values,ref['frequency_CI'].sel(percentile=0.025).values,ref['frequency_CI'].sel(percentile=0.975).values,alpha=0.3)\n",
    "axes[0].fill_between(exp9['latitude'].values,exp9['frequency_CI'].sel(percentile=0.025).values,exp9['frequency_CI'].sel(percentile=0.975).values,alpha=0.3)\n",
    "axes[0].fill_between(exp4['latitude'].values,exp4['frequency_CI'].sel(percentile=0.025).values,exp4['frequency_CI'].sel(percentile=0.975).values,alpha=0.3)\n",
    "axes[0].fill_between(exp8['latitude'].values,exp8['frequency_CI'].sel(percentile=0.025).values,exp8['frequency_CI'].sel(percentile=0.975).values,alpha=0.3)\n",
    "\n",
    "ylim = axes[0].get_ylim()\n",
    "\n",
    "axes[0].plot([ref_latitude,ref_latitude],ylim,color=l1[0].get_color(),linestyle=':',linewidth=1)\n",
    "axes[0].plot([exp9_latitude,exp9_latitude],ylim,color=l2[0].get_color(),linestyle=':',linewidth=1)\n",
    "axes[0].plot([exp4_latitude,exp4_latitude],ylim,color=l3[0].get_color(),linestyle=':',linewidth=1)\n",
    "axes[0].plot([exp8_latitude,exp8_latitude],ylim,color=l4[0].get_color(),linestyle=':',linewidth=1)\n",
    "\n",
    "axes[0].set_ylim(ylim)\n",
    "\n",
    "# length\n",
    "ref['length'].plot(ax=axes[2],linestyle='-')\n",
    "exp9['length'].plot(ax=axes[2],linestyle='--')\n",
    "exp4['length'].plot(ax=axes[2],linestyle=':')\n",
    "exp8['length'].plot(ax=axes[2],linestyle='-.')\n",
    "\n",
    "axes[2].fill_between(ref['latitude'].values,ref['length_CI'].sel(percentile=0.025).values,ref['length_CI'].sel(percentile=0.975).values,alpha=0.3)\n",
    "axes[2].fill_between(exp9['latitude'].values,exp9['length_CI'].sel(percentile=0.025).values,exp9['length_CI'].sel(percentile=0.975).values,alpha=0.3)\n",
    "axes[2].fill_between(exp4['latitude'].values,exp4['length_CI'].sel(percentile=0.025).values,exp4['length_CI'].sel(percentile=0.975).values,alpha=0.3)\n",
    "axes[2].fill_between(exp8['latitude'].values,exp8['length_CI'].sel(percentile=0.025).values,exp8['length_CI'].sel(percentile=0.975).values,alpha=0.3)\n",
    "\n",
    "ylim = axes[2].get_ylim()\n",
    "\n",
    "axes[2].plot([ref_latitude,ref_latitude],ylim,color=l1[0].get_color(),linestyle=':',linewidth=1)\n",
    "axes[2].plot([exp9_latitude,exp9_latitude],ylim,color=l2[0].get_color(),linestyle=':',linewidth=1)\n",
    "axes[2].plot([exp4_latitude,exp4_latitude],ylim,color=l3[0].get_color(),linestyle=':',linewidth=1)\n",
    "axes[2].plot([exp8_latitude,exp8_latitude],ylim,color=l4[0].get_color(),linestyle=':',linewidth=1)\n",
    "\n",
    "axes[2].set_ylim(ylim)\n",
    "\n",
    "\n",
    "# persistence tropical heating\n",
    "sig = (counts_exp9['days'] < CI_ref.sel(percentile=0.025)) + (counts_exp9['days'] > CI_ref.sel(percentile=0.975))\n",
    "\n",
    "C = (counts_exp9['days']-counts_ref['days']).plot(ax=axes[1],levels=np.arange(-3,3.25,0.25),add_colorbar=False,extend='both',cmap=cmocean.cm.delta)\n",
    "                                                \n",
    "sig.astype('double').plot.contourf(ax=axes[1],levels=[0,0.5,1],hatches=['//',''],alpha=0,add_colorbar=False)\n",
    "\n",
    "zeros = xr.zeros_like(counts_ref['latitude']).assign_coords(length=-1000)\n",
    "C0 = xr.concat([zeros,counts_ref['days']],dim='length').plot.contour(ax=axes[1],x='latitude',levels=[1,3,5,7,9,11],colors='k')\n",
    "\n",
    "plt.clabel(C0,fontsize='x-small')\n",
    "\n",
    "\n",
    "# persistence arctic heating\n",
    "sig = (counts_exp4['days'] < CI_ref.sel(percentile=0.025)) + (counts_exp4['days'] > CI_ref.sel(percentile=0.975))\n",
    "\n",
    "C = (counts_exp4['days']-counts_ref['days']).plot(ax=axes[3],levels=np.arange(-3,3.25,0.25),add_colorbar=False,extend='both',cmap=cmocean.cm.delta)\n",
    "                                                \n",
    "sig.astype('double').plot.contourf(ax=axes[3],levels=[0,0.5,1],hatches=['//',''],alpha=0,add_colorbar=False)\n",
    "\n",
    "zeros = xr.zeros_like(counts_ref['latitude']).assign_coords(length=-1000)\n",
    "C0 = xr.concat([zeros,counts_ref['days']],dim='length').plot.contour(ax=axes[3],x='latitude',levels=[1,3,5,7,9,11],colors='k')\n",
    "\n",
    "plt.clabel(C0,fontsize='x-small')\n",
    "\n",
    "\n",
    "for ax in axes:\n",
    "    \n",
    "    ax.set_xlabel('')\n",
    "    ax.set_xlim(0,90)\n",
    "    ax.set_xticks([0,30,60,90])\n",
    "    ax.set_xticks([15,45,75],minor=True)\n",
    "    ax.set_title('')\n",
    "\n",
    "\n",
    "for ax in axes[[1,3]]:\n",
    "    \n",
    "    ax.set_ylabel('Length [days]',fontsize=13)\n",
    "    ax.set_ylim(0.5,14)\n",
    "    ax.set_yticks([3,6,9,12])\n",
    "    ax.set_yticks([1,2,4,5,7,8,10,11,13,14],minor=True)\n",
    "    \n",
    "    xlim = ax.get_xlim()\n",
    "    ax.plot(xlim,[2.5,2.5],linestyle=':',linewidth=1.5,color='m')\n",
    "    ax.set_xlim(xlim)\n",
    "\n",
    "    \n",
    "for ax in axes[[0,2]]:\n",
    "    \n",
    "    ax.xaxis.grid(which='both')\n",
    "        \n",
    "axes[2].set_xlabel('Latitude [°N]',fontsize=13)\n",
    "axes[3].set_xlabel('Latitude [°N]',fontsize=13)\n",
    "axes[0].set_ylabel('Heatwave frequency [days/year]',fontsize=13,labelpad=10)\n",
    "axes[2].set_ylabel('Mean duration [days]',fontsize=13,labelpad=15)\n",
    "\n",
    "\n",
    "\n",
    "axes[1].set_title('Tropical warming',weight='bold',fontsize='smaller')\n",
    "\n",
    "\n",
    "\n",
    "axes[3].set_title('Arctic warming',weight='bold',fontsize='smaller')\n",
    "\n",
    "axes[2].legend(['Reference','Tropical warming','Arctic warming','Combined forcing'],fontsize=11,loc='upper center',ncols=2,columnspacing=0.8,handlelength=1.)\n",
    "    \n",
    "\n",
    "fig.subplots_adjust(0,0,1,1,0.3,0.3)\n",
    "\n",
    "cbar = plt.colorbar(C,ax=axes[[1,3]],orientation='horizontal',shrink=0.9,pad=0.13)\n",
    "cbar.set_label(r'Density difference [year$^{-1}$]',fontsize=13)\n",
    "cbar.set_ticks([-3,-2,-1,0,1,2,3])\n",
    "\n",
    "box = list(axes[2].get_position().bounds)\n",
    "box[1] += 0.11\n",
    "box[3] -= 0.03\n",
    "axes[2].set_position(box)\n",
    "\n",
    "box = list(axes[0].get_position().bounds)\n",
    "box[1] += 0.03\n",
    "box[3] -= 0.03\n",
    "axes[0].set_position(box)\n",
    "\n",
    "trans = mtransforms.ScaledTranslation(-45/72, -20/72, fig.dpi_scale_trans)\n",
    "\n",
    "axes[0].text(-0.1,1.1,'a)',transform=axes[0].transAxes+trans,fontsize='large',va='bottom')\n",
    "axes[2].text(-0.1,1.1,'b)',transform=axes[2].transAxes+trans,fontsize='large',va='bottom')\n",
    "axes[1].text(0.06,1.1,'c)',transform=axes[1].transAxes+trans,fontsize='large',va='bottom')\n",
    "axes[3].text(0.06,1.1,'d)',transform=axes[3].transAxes+trans,fontsize='large',va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5023615-1101-485a-91b6-7c3c362d4456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python-2023]",
   "language": "python",
   "name": "conda-env-python-2023-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
