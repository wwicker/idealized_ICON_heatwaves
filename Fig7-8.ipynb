{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f312586-549a-4144-ae86-d1e0929eabdb",
   "metadata": {},
   "source": [
    "# Hot day persistence\n",
    "\n",
    "## also called duration histogram\n",
    "\n",
    "Resample years for significance testing. This should take account of seasonal cycle. -> Does the order of year matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191fd4d7-a17e-4f8d-892b-b067d8f5e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "import pandas\n",
    "import scipy.signal\n",
    "import cmocean\n",
    "import numba\n",
    "import os\n",
    "\n",
    "work = os.environ.get('WORK')+'/'\n",
    "plt.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b4227-1276-4979-ac75-2ef219d5b2b7",
   "metadata": {},
   "source": [
    "## Mask hot days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15218d6c-63ee-4d1a-8c3c-75aa575deb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load temperature data\n",
    "directory = work+'/wolfgang/ERA5_surf_day_max/'\n",
    "    \n",
    "files = [directory+f for f in os.listdir(directory) if f.startswith('era5_an_t2m_reg05_1h')]\n",
    "files.sort()\n",
    "\n",
    "dist = xr.open_dataarray(work+'/wolfgang/ERA5_surf_day_max/percentiles_31days.nc').sel(p=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec1bb8-525c-4d54-ae5b-e2a3927b972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "failing = []\n",
    "\n",
    "for f in files:\n",
    "    \n",
    "    data = xr.open_dataset(f)['var167']\n",
    "    day = data['time.dayofyear']\n",
    "    tmp = data < dist.sel(dayofyear=day)\n",
    "    \n",
    "    failing.append(tmp)\n",
    "    \n",
    "failing = xr.concat(failing,dim='time')\n",
    "failing = failing.stack(ncells=('lat','lon'))\n",
    "\n",
    "failing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba48a87-eb10-4735-89f6-290f81a645ea",
   "metadata": {},
   "source": [
    "## Count duration year-by-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7c22e3-bd6a-4c02-a10a-a3ab01fd6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def count_duration(array,index):\n",
    "    '''\n",
    "        Count occurence of set of consecutive hot days with certain length\n",
    "        \n",
    "        - first element of occurence counts sets that are longer than max_duration\n",
    "    '''\n",
    "    max_duration = 14\n",
    "    occurence = np.zeros(max_duration+1,np.int_)\n",
    "    count = 0\n",
    "    \n",
    "    for i in index:\n",
    "        if array[i]:\n",
    "            if count > 0:\n",
    "                if count > max_duration:\n",
    "                    occurence[0] += 1\n",
    "                else:\n",
    "                    occurence[count] += 1\n",
    "            \n",
    "            count = 0  \n",
    "            \n",
    "        else:\n",
    "            count +=1\n",
    "            \n",
    "    return occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb8dc8a-1b81-46f6-9bef-1c7291d5314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_cells(array):\n",
    "    '''\n",
    "        Loop counter over ncells\n",
    "    '''\n",
    "    N = len(array.time)\n",
    "    index = np.arange(N)\n",
    "    \n",
    "    # prepare array\n",
    "    len1 = len(count_duration(array.isel(ncells=0).values,index))\n",
    "    hist = np.zeros((len1,len(array.ncells)),np.int_)\n",
    "    \n",
    "    for i in range(len(array.ncells)):\n",
    "        \n",
    "        hist[:,i] = count_duration(array.isel(ncells=i).values,index)\n",
    "        \n",
    "        \n",
    "    # prepare data\n",
    "    length = xr.DataArray(range(1,len1),dims=('length'))\n",
    "    events = xr.DataArray(hist[1:,:],dims=('length','ncells'),coords=dict(length=length))\n",
    "    \n",
    "    days = length * events\n",
    "    \n",
    "    missing_events = xr.DataArray(hist[0,:],dims='ncells')\n",
    "    missing_days = 0.1*N - days.sum('length')\n",
    "    \n",
    "    return xr.Dataset(dict(days=days,missing=missing_days,missing_events=missing_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f6835-7f65-4f29-aa54-a85a2efc844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence = []\n",
    "\n",
    "groups = list(failing.groupby('time.year'))\n",
    "\n",
    "for y, g in groups:\n",
    "    \n",
    "    count = loop_cells(g)\n",
    "    \n",
    "    mean = count.assign_coords(ncells=failing['ncells']).reset_index('ncells').unstack().mean('lon')\n",
    "    \n",
    "    persistence.append(mean.assign_coords(year=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56aae49-f472-45d8-88a6-5f5508760840",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence = xr.concat(persistence, dim='year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a60eb19-d3a4-4080-bde0-c02c0d457596",
   "metadata": {},
   "source": [
    "## Significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328081c-f433-47b4-b5c3-3b201849d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.guvectorize(\n",
    "    \"(float64[:],float64[:],float64[:,:])\",\n",
    "    \"(n), (m) -> (m,n)\",\n",
    "    forceobj=True\n",
    ")\n",
    "def random_sample(a,nb,out):\n",
    "    '''\n",
    "        Draw len(nb) random samples from array a\n",
    "        'ziehen mit zuruecklegen'\n",
    "        \n",
    "        - nb is a dummy array to get dimension size\n",
    "    '''\n",
    "    lt = len(a)\n",
    "    variates = scipy.stats.uniform.rvs(0,lt,lt*len(nb))\n",
    "    variates = variates.astype(int).reshape(len(nb),lt)\n",
    "    out[:,:] = a[variates]\n",
    "\n",
    "    \n",
    "@numba.guvectorize(\n",
    "    \"(float64[:],float64[:],float64[:])\",\n",
    "    \"(n), (m) -> (m)\",\n",
    "    forceobj=True\n",
    ")    \n",
    "def icdf(a,p,out):\n",
    "    '''\n",
    "        Inverse empirical cummulative distribution function of array at percentiles p\n",
    "    '''\n",
    "    sort = np.sort(a)\n",
    "    out[:] = sort[np.int64(p*len(a))]\n",
    "    \n",
    "    \n",
    "def t_statistic(x1,x2,dim):\n",
    "    '''\n",
    "        T-statistic for the difference of the mean for two samples of equal length\n",
    "    '''\n",
    "    diff = x1.mean(dim) - x2.mean(dim)\n",
    "    err = x1.var(dim) + x2.var(dim)\n",
    "    err = np.sqrt(err/len(x1[dim]))\n",
    "    return diff / err\n",
    "\n",
    "\n",
    "def parametric_bootstrap(sample1,sample2,nb=1000,confid=0.05):\n",
    "    '''\n",
    "        Test ensemble mean difference\n",
    "    '''\n",
    "    # Produce control samples that fullfill the Null hypothesis\n",
    "    c1 = sample1 - sample1.mean('year')\n",
    "    c2 = sample2 - sample2.mean('year')\n",
    "    \n",
    "    # Resample control\n",
    "    bootstrap = xr.DataArray(np.arange(nb),dims=('random'))\n",
    "    c1 = xr.apply_ufunc(random_sample,\n",
    "                         *(c1,bootstrap),\n",
    "                         input_core_dims=[['year'],['random']],\n",
    "                         output_core_dims=[['random','year']],\n",
    "                         dask='parallelized',\n",
    "                         output_dtypes=[[c1.dtype]])\n",
    "    c2 = xr.apply_ufunc(random_sample,\n",
    "                         *(c2,bootstrap),\n",
    "                         input_core_dims=[['year'],['random']],\n",
    "                         output_core_dims=[['random','year']],\n",
    "                         dask='parallized',\n",
    "                         output_dtypes=[[c1.dtype]])\n",
    "    \n",
    "    # t statistic for the resampled data\n",
    "    dist = t_statistic(c1,c2,'year')\n",
    "    \n",
    "    # emperical cumulative distribution function\n",
    "    p = xr.DataArray(np.linspace(0,0.999,1000),dims=('percentile'))\n",
    "    dist = xr.apply_ufunc(icdf,\n",
    "                          *(dist,p),\n",
    "                          input_core_dims=[['random'],['percentile']],\n",
    "                          output_core_dims=[['percentile']],\n",
    "                          dask='parallelized',\n",
    "                          output_dtypes=[[dist.dtype]])\n",
    "    dist['percentile'] = p\n",
    "    \n",
    "    # check whether Null hypothesis can be rejected\n",
    "    t = t_statistic(sample1,sample2,'year')\n",
    "    sig = np.add(t < dist.sel(percentile=confid/2,method='nearest'), \n",
    "                 t > dist.sel(percentile=1-confid/2,method='nearest'))\n",
    "    \n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39264ed1-0499-4865-975d-f2d2a2242287",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = persistence['days']\n",
    "\n",
    "\n",
    "clim = days.sel(year=slice(1979,2022))\n",
    "early = days.sel(year=slice(1979,2000))\n",
    "late = days.sel(year=slice(2001,2022))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7214b6-50ba-498f-bb34-331c15c1d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = parametric_bootstrap(early-clim.mean('year'),late-clim.mean('year'),nb=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc51b91f-a7b5-4ee5-90d2-e12f9f707dce",
   "metadata": {},
   "source": [
    "## Zonal-mean heatwave frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54cb8dc-f2c1-4ff7-90d8-162bb643ee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_metrics(files):\n",
    "    '''\n",
    "        Zonal mean heatwave frequency and median duration for each year\n",
    "        \n",
    "        - year-round\n",
    "        - one file for each year\n",
    "    '''\n",
    "    frequency = []\n",
    "    length = []\n",
    "    \n",
    "    for f in files:\n",
    "        print(f)\n",
    "        \n",
    "        data = pandas.read_json(f)\n",
    "        data['year'] = data['start'].apply(lambda x: int(x[0:4]))\n",
    "        \n",
    "        frequency.append(data.groupby(['lat','lon','year'])['length'].sum().to_xarray().mean('lon'))\n",
    "        length.append(data.groupby(['lat','year'])['length'].median().to_xarray())\n",
    "        \n",
    "    frequency = xr.concat(frequency,dim='year')\n",
    "    length = xr.concat(length,dim='year')\n",
    "        \n",
    "    return xr.Dataset(dict(events=frequency,days=length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8517e92-1eb9-4d0c-93af-56b83abd6d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = work+'wolfgang/ERA5_surf_day_max/heat_1979-2022/'\n",
    "\n",
    "files = [directory+f for f in os.listdir(directory) if f.endswith('json')]\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724808aa-db1d-4ef2-b7e9-681940866c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "surf = improved_metrics(files)\n",
    "surf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233fb04d-dcca-4634-84c8-ce2f8c0ca9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = surf['events']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82853c22-19a0-4bbd-97f9-19ae2a52168d",
   "metadata": {},
   "source": [
    "## Figure 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda2a161-689f-4152-b693-4791d9f7096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2,ncols=1,figsize=(6,8))\n",
    "\n",
    "## heatwave frequency\n",
    "\n",
    "C = frequency.plot(ax=axes[0],x='lat',cmap=cmocean.cm.amp,levels=np.arange(0,37),add_colorbar=False)\n",
    "\n",
    "ylim = axes[0].get_ylim()\n",
    "\n",
    "#axes[0].plot([-46.5,-46.5],ylim,'-k')\n",
    "#axes[0].plot([-58.5,-58.5],ylim,'--k')\n",
    "\n",
    "axes[0].set_ylabel('Year')\n",
    "axes[0].set_xlabel('Latitude [°N]')\n",
    "axes[0].set_xticks([-90,-60,-30,0,30,60,90])\n",
    "\n",
    "cbar1 = plt.colorbar(C,ax=axes[0])\n",
    "cbar1.set_label('Heatwave frequency [days / year]',fontsize=12)\n",
    "\n",
    "\n",
    "# hot days\n",
    "\n",
    "C = (late.mean('year')-early.mean('year')).plot(ax=axes[1],levels=np.arange(-3,3.25,0.25),add_colorbar=False,extend='both',cmap=cmocean.cm.delta)\n",
    "                                                \n",
    "sig.astype('double').plot.contourf(ax=axes[1],levels=[0,0.5,1],hatches=['//',''],alpha=0,add_colorbar=False)\n",
    "\n",
    "zeros = xr.zeros_like(days['lat']).assign_coords(length=-1000)\n",
    "C0 = xr.concat([zeros,clim.mean('year')],dim='length').plot.contour(ax=axes[1],x='lat',levels=[1,3,5,7,9,11,13,15,17],colors='w')\n",
    "\n",
    "plt.clabel(C0,fontsize='x-small')\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(C,ax=axes[1],orientation='vertical')\n",
    "cbar.set_label(r'Density difference [year$^{-1}$]',fontsize=13)\n",
    "cbar.set_ticks([-3,-2,-1,0,1,2,3])\n",
    "\n",
    "axes[1].set_ylabel('Length [days]',fontsize=13)\n",
    "axes[1].set_ylim(0.5,14)\n",
    "axes[1].set_yticks([3,6,9,12])\n",
    "axes[1].set_yticks([1,2,4,5,7,8,10,11,13,14],minor=True)\n",
    "    \n",
    "xlim = axes[1].get_xlim()\n",
    "axes[1].plot(xlim,[2.5,2.5],linestyle='--',linewidth=1.5,color='m')\n",
    "axes[1].set_xticks([-90,-60,-30,0,30,60,90])\n",
    "axes[1].set_xlim(xlim)\n",
    "\n",
    "axes[1].set_xlabel('Latitude [°N]',fontsize=13)\n",
    "\n",
    "axes[1].set_title(r'2001-2022 $-$ 1979-2000',weight='bold',fontsize=14)\n",
    "\n",
    "fig.subplots_adjust(0,0,1,1,0.3,0.3)\n",
    "\n",
    "trans = mtransforms.ScaledTranslation(-45/72, -20/72, fig.dpi_scale_trans)\n",
    "\n",
    "axes[0].text(-0.06,1.06,'a)',transform=axes[0].transAxes+trans,fontsize='large',va='bottom')\n",
    "axes[1].text(-0.06,1.06,'b)',transform=axes[1].transAxes+trans,fontsize='large',va='bottom')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb0464e-06cd-4926-a407-af5860877762",
   "metadata": {},
   "source": [
    "## Process hovmoeller plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29225849-7ba4-48bd-b1fd-8ef92f6bda83",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = work+'wolfgang/ERA5_surf_day_max/hovmoeller/'\n",
    "\n",
    "years = range(2001,2023)\n",
    "\n",
    "files = [[directory+f for f in os.listdir(directory) if f.endswith('-58.5N_%d.nc'%y)] for y in years]\n",
    "\n",
    "ds = xr.open_mfdataset(files,combine='nested',concat_dim=('onset','onset'))\n",
    "\n",
    "late_day = ds['anomaly'].where(ds['onset.month'].isin(range(1,13)),drop=True).mean('onset').compute()\n",
    "\n",
    "#late_day = late_day.interp(step=np.arange(-5,10,2/24),method='cubic')\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc49e2-0dec-45c6-93ff-f53ea376dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = work+'wolfgang/ERA5_surf_day_max/hovmoeller/'\n",
    "\n",
    "years = range(1979,2001)\n",
    "\n",
    "files = [[directory+f for f in os.listdir(directory) if f.endswith('-46.5N_%d.nc'%y)] for y in years]\n",
    "\n",
    "ds = xr.open_mfdataset(files,combine='nested',concat_dim=('onset','onset'))\n",
    "\n",
    "early_day = ds['anomaly'].where(ds['onset.month'].isin([range(1,13)]),drop=True).mean('onset').compute()\n",
    "\n",
    "#early_day = early_day.interp(step=np.arange(-5,10,1/24),method='cubic')\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea48ed-e357-4499-ac79-fb5966d4f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.guvectorize(   \n",
    "    \"(float64[:],int16,complex128[:])\",\n",
    "    \"(n), () -> (n)\",\n",
    "    forceobj=True\n",
    ")\n",
    "def _hilbert(y,N,out):\n",
    "    '''\n",
    "        Analytic signal using the Hilbert transform technique (Marple, 1999)\n",
    "    '''\n",
    "    # Check whether the signal has even or odd length\n",
    "    if N%2==0:\n",
    "        a = int(N/2)\n",
    "    else:\n",
    "        if N>1:\n",
    "            a = int((N-1)/2)\n",
    "        else:\n",
    "            a = 0\n",
    "        \n",
    "    # FFT of y\n",
    "    z = np.fft.fft(y)\n",
    "    \n",
    "    # Zero-out the negative frequencies\n",
    "    z[a+1:N] = 0\n",
    "    # Double the positive frequencies except from the 0th and (N/2)th ones\n",
    "    z = 2*z\n",
    "    z[0] = z[0]/2\n",
    "    if N%2==0: \n",
    "        # For the even-length case, we also have the Nyquist frequency in the spectrum. \n",
    "        # This is shared between the positive and negative frequencies so we need to keep it once (see Marple 1999). \n",
    "        # For odd lengths, there is no Nyquist frequency in the spectrum.\n",
    "        z[a] = z[a]/2\n",
    "\n",
    "    # Inverse FFT to get the analytic signal\n",
    "    out[:] = np.fft.ifft(z)\n",
    "    \n",
    "    \n",
    "@numba.vectorize([numba.float64(numba.float64, numba.float64)])\n",
    "def _rad_diff(a,b):\n",
    "    '''\n",
    "        In cases where the upstream and downstream phase differ more than pi or -pi, add/subtract 2pi where needed.\n",
    "    '''\n",
    "    diff = a - b\n",
    "    if diff > np.pi:\n",
    "        diff -= 2*np.pi\n",
    "    elif diff < -np.pi:\n",
    "        diff += 2*np.pi\n",
    "        \n",
    "    return diff\n",
    "    \n",
    "    \n",
    "@numba.guvectorize(   \n",
    "    \"(float64[:],float64[:])\",\n",
    "    \"(n) -> (n)\",\n",
    "    forceobj=True\n",
    ")    \n",
    "def _finite_difference(a,out):\n",
    "    '''\n",
    "        Use centered differences in the interior, one-sided differences at the boundaries\n",
    "    '''\n",
    "    out[1:-1] = _rad_diff(a[2:],a[:-2])/2.\n",
    "    out[-1] = _rad_diff(a[-1],a[-2])\n",
    "    out[0] = _rad_diff(a[1],a[0])\n",
    "    \n",
    "    \n",
    "@numba.guvectorize(   \n",
    "    \"(float64[:],float64[:],float64[:])\",\n",
    "    \"(n), (n) -> (n)\",\n",
    "    forceobj=True\n",
    ")\n",
    "def _fft_filter(y,mask,out):\n",
    "    '''\n",
    "        Filter by multiplication in spectral space\n",
    "    '''\n",
    "    z = scipy.fft.fft(y)\n",
    "    z = z * mask\n",
    "    out[:] = scipy.fft.ifft(z)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5126c0-a6ad-4a63-87d9-1fe34c5824f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering(da,kmin=2,kmax=10,alpha=0.5):\n",
    "    '''\n",
    "    '''\n",
    "    # prepare mask\n",
    "    N = len(da.lon)\n",
    "    taper = scipy.signal.tukey(kmax-kmin+1,alpha=alpha)\n",
    "    mask = np.zeros(N)\n",
    "    mask[kmin:kmax+1] = taper\n",
    "    mask = xr.DataArray(mask,dims=('freq'))\n",
    "    \n",
    "    # multiply data with mask in spectral space\n",
    "    filtered = xr.apply_ufunc(_fft_filter,\n",
    "                              *(da,mask),\n",
    "                              input_core_dims=[['lon'],['freq']],\n",
    "                              output_core_dims=[['lon']],\n",
    "                              dask='parallelized',\n",
    "                              output_dtypes=[da.dtype]\n",
    "                             )\n",
    "    \n",
    "    # Since the ignored negative frequencies would contribute the same as the positive ones\n",
    "    filtered *= 2 \n",
    "    \n",
    "    return filtered\n",
    "\n",
    "def envelope_phase(da):\n",
    "    '''\n",
    "        Absolute value and phase angle of the complex signal\n",
    "    '''\n",
    "    sig = xr.apply_ufunc(_hilbert,\n",
    "                         *(da,len(da.lon)),\n",
    "                         input_core_dims=[['lon'],[]],\n",
    "                         output_core_dims=[['lon']],\n",
    "                         dask='parallelized',\n",
    "                         output_dtypes=[np.dtype('complex128')]\n",
    "                        )\n",
    "\n",
    "    env = np.abs(sig)\n",
    "    phase = np.arctan2(np.imag(sig),np.real(sig))\n",
    "    \n",
    "    return xr.Dataset(dict(env=env,phase=phase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a0f50-0fc0-479b-a3e4-05a6880ea584",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = filtering(late_day,kmin=2,kmax=15,alpha=0.5)\n",
    "ds = envelope_phase(da)\n",
    "group = envelope_phase(ds['env'])\n",
    "\n",
    "late = xr.Dataset(dict(ta=late_day,wave=ds['phase'],envelope=ds['env'],group=group['phase']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64af70d5-99ef-4961-b63f-5672ecc702e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab32ea5-7caf-4a7b-86c8-9aa768c8da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = filtering(early_day,kmin=2,kmax=15,alpha=0.5)\n",
    "ds = envelope_phase(da)\n",
    "group = envelope_phase(ds['env'])\n",
    "\n",
    "early = xr.Dataset(dict(ta=early_day,wave=ds['phase'],envelope=ds['env'],group=group['phase']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a6763-2d4a-461b-902d-493774eba435",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = lambda da: np.mod(da,2*np.pi)-np.pi\n",
    "\n",
    "early['wave_shifted'] = shift(early['wave'])\n",
    "late['wave_shifted'] = shift(late['wave'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8205c69-b3e3-4e7a-8be7-3319a7843177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a34393fd-9487-4ffa-b103-cc90fa6dc602",
   "metadata": {},
   "source": [
    "## Figure 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656471be-cc4d-49ae-9876-2145d55a5a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3,figsize=(8,10))\n",
    "\n",
    "\n",
    "C = early['ta'].plot(ax=axes[0],levels=np.linspace(-3,3,21),extend='both',add_colorbar=False)\n",
    "\n",
    "early['wave'].where((early['envelope']>0.19)*(np.abs(early['wave'])<3)).plot.contour(ax=axes[0],levels=[0,],colors='k',linestyles=':',alpha=0.6)\n",
    "early['wave_shifted'].where((early['envelope']>0.19)*(np.abs(early['wave_shifted'])<2)).plot.contour(ax=axes[0],levels=[0,],colors='k',linestyles=':',alpha=0.6)\n",
    "early['group'].where(early['envelope']>0.19).plot.contour(ax=axes[0],levels=[0,],colors='k',alpha=0.6)\n",
    "\n",
    "C0 = early['envelope'].plot.contour(ax=axes[0],levels=[0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6],colors='w',alpha=0.8)\n",
    "\n",
    "plt.clabel(C0,levels=[0.4,0.8,1.2],fontsize='x-small',colors='k')\n",
    "\n",
    "\n",
    "\n",
    "late['ta'].plot(ax=axes[1],levels=np.linspace(-3,3,21),extend='both',add_colorbar=False)\n",
    "\n",
    "late['wave'].where((late['envelope']>0.19)*(np.abs(late['wave'])<1)).plot.contour(ax=axes[1],levels=[0,],colors='k',linestyles=':',alpha=0.6)\n",
    "late['wave_shifted'].where((late['envelope']>0.19)*(np.abs(late['wave_shifted'])<1)).plot.contour(ax=axes[1],levels=[0,],colors='k',linestyles=':',alpha=0.6)\n",
    "late['group'].where(late['envelope']>0.19).plot.contour(ax=axes[1],levels=[0,],colors='k',alpha=0.6)\n",
    "\n",
    "C0 = late['envelope'].plot.contour(ax=axes[1],levels=[0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6],colors='w',alpha=0.8)\n",
    "\n",
    "plt.clabel(C0,levels=[0.4,0.8,1.2],fontsize='x-small',colors='k')\n",
    "\n",
    "\n",
    "late['wave'].where((late['envelope']>0.19)*(np.abs(late['wave'])<1)).plot.contour(ax=axes[2],levels=[0,],colors='#1f77b4',linestyles=':')\n",
    "late['wave_shifted'].where((late['envelope']>0.19)*(np.abs(late['wave_shifted'])<1)).plot.contour(ax=axes[2],levels=[0,],colors='#1f77b4',linestyles=':')\n",
    "late['group'].where(late['envelope']>0.19).plot.contour(ax=axes[2],levels=[0,],colors='#1f77b4',linewidths=1)\n",
    "late['ta'].plot.contour(ax=axes[2],levels=[0],colors='#1f77b4')\n",
    "\n",
    "early['wave'].where((early['envelope']>0.19)*(np.abs(early['wave'])<1)).plot.contour(ax=axes[2],levels=[0,],colors='#ff7f0e',linestyles=':')\n",
    "early['wave_shifted'].where((early['envelope']>0.19)*(np.abs(early['wave_shifted'])<1)).plot.contour(ax=axes[2],levels=[0,],colors='#ff7f0e',linestyles=':')\n",
    "early['group'].where(early['envelope']>0.19).plot.contour(ax=axes[2],levels=[0,],colors='#ff7f0e',linewidths=1)\n",
    "early['ta'].plot.contour(ax=axes[2],levels=[0],colors='#ff7f0e')\n",
    "\n",
    "\n",
    "l1 = axes[2].plot([],[],color='#1f77b4')\n",
    "l2 = axes[2].plot([],[],color='#ff7f0e')\n",
    "\n",
    "axes[2].legend([*l1,*l2],['1979-2000','2001-2022'],loc='upper left',fontsize=10)\n",
    "\n",
    "\n",
    "for ax in axes:\n",
    "    \n",
    "    ax.plot(ax.get_xlim(),[0,0],linestyle=':',linewidth=0.5,color='k')\n",
    "    ax.plot([0,0],ax.get_ylim(),linestyle=':',linewidth=0.5,color='k')\n",
    "\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_ylim(-4,10)\n",
    "    ax.set_xticks([-180,-120,-60,0,60,120,180])\n",
    "    ax.set_yticks([-4,0,4,8],minor=False)\n",
    "    \n",
    "    \n",
    "axes[0].set_title('1979-2000',weight='bold',fontsize='smaller')\n",
    "axes[1].set_title('2001-2022',weight='bold',fontsize='smaller')\n",
    "\n",
    "axes[2].set_xlabel('Relative longitude [°E] relative to grid point')\n",
    "axes[1].set_ylabel('Lag [days] relative to heatwave onset',fontsize=16)\n",
    "\n",
    "axes[0].set_xticklabels([])\n",
    "axes[1].set_xticklabels([])\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(C,ax=axes[:2],aspect=30)\n",
    "cbar.set_label('Day max T2m anomaly from 90th percentile')\n",
    "\n",
    "box = list(axes[2].get_position().bounds)\n",
    "box[2] = axes[1].get_position().bounds[2]\n",
    "axes[2].set_position(box)\n",
    "\n",
    "trans = mtransforms.ScaledTranslation(-45/72, -20/72, fig.dpi_scale_trans)\n",
    "\n",
    "axes[0].text(-0.1,1.06,'a)',transform=axes[0].transAxes+trans,fontsize='large',va='bottom')\n",
    "axes[1].text(-0.1,1.06,'b)',transform=axes[1].transAxes+trans,fontsize='large',va='bottom')\n",
    "axes[2].text(-0.1,1.06,'c)',transform=axes[2].transAxes+trans,fontsize='large',va='bottom')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be757967-de99-44a3-86a4-4ad93e5484d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ca9c1-fcb2-41a8-9c29-105a5452fd98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa5be50-7afd-4f61-98ae-306fce67e42c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python-2023]",
   "language": "python",
   "name": "conda-env-python-2023-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
